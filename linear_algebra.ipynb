{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXtZ91QxJq69Py1d3liUsE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeabwang/tensorflow/blob/main/linear_algebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bdQsZ1H9zVHE"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# matrix multiplication // not element wise\n",
        "# For this to happen the number of columns in the first row should be equal to the number of columns in the second row.\n",
        "#  2x3 and 3x2 - valid\n",
        "# 2x3 and 2x3 - invalid\n",
        "# In case of 3d matrix, we are going to do the multiplication the first batch of the first matrix to the first batch of the second matrix\n",
        "# is_sparse is used when we have a matrix full of  zeros and we are telling our processor so that it compute the operations efficienctly\n",
        "\n",
        "x1 = tf.constant([[1,2,0],\n",
        "                [3,5,-1]])\n",
        "x2 = tf.constant([[1,2,0],\n",
        "                 [3,5,-1],\n",
        "                [3,5,-1]])\n",
        "\n",
        "tf.linalg.matmul(x1,x2)\n",
        "print(x1@x2)\n",
        "tf.linalg.matmul(x1,x2,transpose_b=False, transpose_a = False,adjoint_a=False, a_is_sparse=False,output_type=None, name=None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHVfDLOizcMm",
        "outputId": "c8ac57ec-dca1-4838-a79c-d450058eec1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[ 7 12 -2]\n",
            " [15 26 -4]], shape=(2, 3), dtype=int32)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
              "array([[ 7, 12, -2],\n",
              "       [15, 26, -4]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transposing\n",
        "tf.transpose(x1)\n",
        "tf.transpose(x2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KDMK_Cn0SL3",
        "outputId": "e1d7976c-099d-4659-b4cf-8b47306f9ec6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
              "array([[ 1,  3,  3],\n",
              "       [ 2,  5,  5],\n",
              "       [ 0, -1, -1]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjoint\n",
        "tf.linalg.adjoint(x1)"
      ],
      "metadata": {
        "id": "PMWugMAi6AmE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecde7338-87a6-4711-c93a-04f62c9e6213"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
              "array([[ 1,  3],\n",
              "       [ 2,  5],\n",
              "       [ 0, -1]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Band: A band matrix is a special type of matrix that has non-zero entries concentrated around the main diagonal, with all other entries being zero.\n",
        "# tf.linalg.band_part(input, lwr_bound, upr_bound)\n",
        "# The main condition we are checking is\n",
        "# (lwr_bound < 0 or m-n <=lower) and (upper<0 or n-m <= upper)\n",
        "\n",
        "# Special cases\n",
        "# lwr_bound 0 and upr_bound -1 -> Upper triangle part\n",
        "# lwr_bound -1 and upr_bound 0 -> Lower triangle part\n",
        "# lwr_bound 0 and upr_bound 0 ->  Diagonal\n",
        "\n",
        "x = tf.constant([[1, 2, 3],\n",
        "                 [4, 5, 6],\n",
        "                 [7, 8, 9]])\n",
        "\n",
        "tf.linalg.band_part(x, 0, -1)\n",
        "# tf.linalg.band_part(x, -1, 0)\n",
        "#tf.linalg.band_part(x, 0, 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FnNN34DAokJ",
        "outputId": "ef066c84-801f-4a3e-e85a-83a2d67b906c"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
              "array([[1, 0, 0],\n",
              "       [0, 5, 0],\n",
              "       [0, 0, 9]], dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inverse\n",
        "tens_matrix = tf.constant([[1, 2, 3],\n",
        "                 [0, 1, 4],\n",
        "                 [5, 6, 0]], dtype=tf.float32)\n",
        "# x= tf.cast(x,tf.float32)\n",
        "tensor_inverse = tf.linalg.inv(tens_matrix)\n",
        "tens_matrix@tensor_inverse # the result will be an identity matrix\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XysgUpu-HM97",
        "outputId": "82546cfb-aadc-496d-d263-34f6511befbf"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
              "array([[ 1.0000010e+00, -1.4305115e-06,  0.0000000e+00],\n",
              "       [ 0.0000000e+00,  1.0000000e+00,  0.0000000e+00],\n",
              "       [ 0.0000000e+00,  0.0000000e+00,  1.0000000e+00]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# svd(singular value decomposition) -- A=UΣV^T\n",
        "# The main goal here is to eliminate the part which contain less important information\n",
        "# Σ(s) - Singular value - epresent the magnitude of the stretching (or shrinking) applied by A\n",
        "      # To calculate the singular value\n",
        "      # Transpose the matrix.\n",
        "      # Multiply the matrix with its transpose.\n",
        "      # Find the eigenvalues of the resulting matrix.\n",
        "      # Take the square roots of those eigenvalues to get the singular values.\n",
        "\n",
        "# U - Left singular values\n",
        "      #(A*AT−λI)u=0 --> (A * A Transpose - lamda(identity)) = 0\n",
        "\n",
        "# V - Right singular value\n",
        "      # (AT*A -λI)v = 0 --> (A Transpose * A - lamda(identity)) = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "bREBJZSDMXdA"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tens_matrix = tf.constant([[1, 2, 3],\n",
        "                 [0, 1, 4],\n",
        "                 [5, 6, 0]], dtype=tf.float32)\n",
        "\n",
        "tf.linalg.svd(tens_matrix, full_matrices=False, compute_uv=True, name=None)\n",
        "s,u,v = tf.linalg.svd(tens_matrix)\n",
        "print(s)\n",
        "print(u)\n",
        "print(v)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYI-wO02Ww8u",
        "outputId": "d3564114-9a8c-4cf2-c87f-56592b2a9a97"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([8.27884    4.8435745  0.02493798], shape=(3,), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.33780327  0.5131884  -0.7890036 ]\n",
            " [ 0.19885956  0.78044283  0.5927598 ]\n",
            " [ 0.9199693  -0.3571372   0.16158365]], shape=(3, 3), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[ 0.5964182  -0.26271883  0.7584618 ]\n",
            " [ 0.7723647  -0.06937096 -0.63137984]\n",
            " [ 0.21849047  0.9623757   0.16154054]], shape=(3, 3), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# einsum - Einstein summation notation\n",
        "# tf.einsum is function that allows for efficient computation of various tensor operations\n",
        "#  particularly those involving summation, contraction, and index manipulation.\n",
        "# So we can say its just a way of making these operations clean and efficient\n",
        "\n",
        "x1 = tf.constant([\n",
        "    [2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]])\n",
        "x2 = tf.constant([\n",
        "    [2,9,0,3,0],\n",
        "    [3,6,8,-2,2],\n",
        "    [1,3,5,0,1],\n",
        "    [3,0,2,0,5]])\n",
        "\n",
        "print(x1.shape)\n",
        "print(x2.shape , \"\\n\")\n",
        "print(\"Multiplication with mul = \\n\")\n",
        "x_mult = tf.linalg.matmul(x1,x2)\n",
        "print(x_mult , \"\\n\")\n",
        "\n",
        "print(\"Multioplication with einsum = \\n\")\n",
        "x_ein = tf.einsum('ij,jk -> ik', x1,x2)\n",
        "print(x_ein)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP5pw_rpXOe-",
        "outputId": "eb960273-cf86-4f34-9cd4-e03f0b73ca18"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "(4, 5) \n",
            "\n",
            "Multiplication with mul = \n",
            "\n",
            "tf.Tensor(\n",
            "[[32 66 72 -6 26]\n",
            " [ 9 12  0 10 13]\n",
            " [21 51 60 -7 14]], shape=(3, 5), dtype=int32) \n",
            "\n",
            "Multioplication with einsum = \n",
            "\n",
            "tf.Tensor(\n",
            "[[32 66 72 -6 26]\n",
            " [ 9 12  0 10 13]\n",
            " [21 51 60 -7 14]], shape=(3, 5), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#einsum element wise operation - They must have same shape\n",
        "\n",
        "x1 = tf.constant([\n",
        "    [2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]])\n",
        "x2 = tf.constant([\n",
        "    [2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0],])\n",
        "\n",
        "print(x1.shape)\n",
        "print(x2.shape , \"\\n\")\n",
        "\n",
        "print(\"Element wise multiplication = \\n\")\n",
        "el_mul = x1*x2\n",
        "print(el_mul, \"\\n\")\n",
        "\n",
        "el_ein = tf.einsum('ij, ij -> ij', x1,x2)\n",
        "print(el_ein)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPmoxJQ_gC_k",
        "outputId": "ff542eb1-e10c-497e-def5-a57f2215c97e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 4)\n",
            "(3, 4) \n",
            "\n",
            "Element wise multiplication = \n",
            "\n",
            "tf.Tensor(\n",
            "[[  4  54   0   6]\n",
            " [  6 -12  16  -6]\n",
            " [  1  15  20   0]], shape=(3, 4), dtype=int32) \n",
            "\n",
            "tf.Tensor(\n",
            "[[  4  54   0   6]\n",
            " [  6 -12  16  -6]\n",
            " [  1  15  20   0]], shape=(3, 4), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# einsum matrix transpose\n",
        "x1 = tf.constant([\n",
        "    [2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]])\n",
        "\n",
        "print(\"Matrix Transpose = \\n\")\n",
        "el_trn = tf.transpose(x1)\n",
        "print(el_trn, \"\\n\")\n",
        "\n",
        "print(\"Transpose with ein = \\n\")\n",
        "ein_trn = tf.einsum('ij -> ji', x1)\n",
        "print(ein_trn, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APF6I8glhoe1",
        "outputId": "0b1004c6-cf3b-408d-8438-7d5822eb676b"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Transpose = \n",
            "\n",
            "tf.Tensor(\n",
            "[[ 2  2  1]\n",
            " [ 6 -2  5]\n",
            " [ 4  2  4]\n",
            " [ 2  3  0]], shape=(4, 3), dtype=int32) \n",
            "\n",
            "Transpose with ein = \n",
            "\n",
            "tf.Tensor(\n",
            "[[ 2  2  1]\n",
            " [ 6 -2  5]\n",
            " [ 4  2  4]\n",
            " [ 2  3  0]], shape=(4, 3), dtype=int32) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ensum with 3d tensor\n",
        "# When we are dealing with a 3d matrix multiplication the batch size of the two must be equal\n",
        "# a = b,i,j amd x = b,i,j  the batch of the a and the x matrix has to be equal\n",
        "# Then we will perform a batch wise multiplication\n",
        "\n",
        "# Element wise operation\n",
        "x1 = tf.constant([\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]],\n",
        "\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]]\n",
        "])\n",
        "x2 = tf.constant([\n",
        "    [[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0]]\n",
        "\n",
        "    ,[[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0]]\n",
        "])\n",
        "\n",
        "print(\"3D multiplication with normal = \\n\")\n",
        "threed_mul = x1*x2\n",
        "print(threed_mul, \"\\n\")\n",
        "\n",
        "print(\"3D multiplication with einsum = \\n\")\n",
        "three_einsum = tf.einsum('bij,bij -> bij', x1,x2)\n",
        "print(three_einsum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbtX7Xq4jTYb",
        "outputId": "529fea0f-08e9-4d14-c9c7-35cb02cf2d7b"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3D multiplication with normal = \n",
            "\n",
            "tf.Tensor(\n",
            "[[[  4  54   0   6]\n",
            "  [  6 -12  16  -6]\n",
            "  [  1  15  20   0]]\n",
            "\n",
            " [[  4  54   0   6]\n",
            "  [  6 -12  16  -6]\n",
            "  [  1  15  20   0]]], shape=(2, 3, 4), dtype=int32) \n",
            "\n",
            "3D multiplication with einsum = \n",
            "\n",
            "tf.Tensor(\n",
            "[[[  4  54   0   6]\n",
            "  [  6 -12  16  -6]\n",
            "  [  1  15  20   0]]\n",
            "\n",
            " [[  4  54   0   6]\n",
            "  [  6 -12  16  -6]\n",
            "  [  1  15  20   0]]], shape=(2, 3, 4), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3d matrix multiplication - it has to be\n",
        "# x1 = b,i,j and x2 = b,i,j   -- b and b has to be equal and i of the x1 has to be equal to j of x2.\n",
        "# b (batch dimension) must be equal in both tensors.\n",
        "# j (third dimension of x1) must match i (second dimension of x2).\n",
        "# The result will have the shape [b, i, k],\n",
        "# where i is the second dimension of x1 and k is the third dimension of x2.\n",
        "\n",
        "# x1. shape = 2,3,4\n",
        "x1 = tf.constant([\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]],\n",
        "\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]]\n",
        "])\n",
        "\n",
        "# x2.shape = 2,4,3\n",
        "x2 = tf.constant([\n",
        "    [[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0],\n",
        "    [1,3,5,0]]\n",
        "\n",
        "    ,[[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0],\n",
        "    [1,3,5,0]]\n",
        "])\n",
        "\n",
        "#three_mx_mul = x1@x2\n",
        "three_mx_mul = tf.linalg.matmul(x1,x2)\n",
        "print( \"The matrix multiplication result will be \\n\" ,three_mx_mul, \"\\n\")\n",
        "\n",
        "three_einsum = tf.einsum('bij,bjk -> bik', x1,x2)\n",
        "print( \"The einsum matrix multiplication result will be \\n\" ,three_einsum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6OZMYJnmAQL",
        "outputId": "76298d58-f415-47be-a3d5-5f2726b73696"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The matrix multiplication result will be \n",
            " tf.Tensor(\n",
            "[[[28 72 78 -6]\n",
            "  [ 3 21  9 10]\n",
            "  [21 51 60 -7]]\n",
            "\n",
            " [[28 72 78 -6]\n",
            "  [ 3 21  9 10]\n",
            "  [21 51 60 -7]]], shape=(2, 3, 4), dtype=int32) \n",
            "\n",
            "The einsum matrix multiplication result will be \n",
            " tf.Tensor(\n",
            "[[[28 72 78 -6]\n",
            "  [ 3 21  9 10]\n",
            "  [21 51 60 -7]]\n",
            "\n",
            " [[28 72 78 -6]\n",
            "  [ 3 21  9 10]\n",
            "  [21 51 60 -7]]], shape=(2, 3, 4), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# summing up 3d arrays\n",
        "\n",
        "# x1. shape = 2,3,4\n",
        "x1 = tf.constant([\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]],\n",
        "\n",
        "    [[2,6,4,2],\n",
        "    [2,-2,2,3],\n",
        "    [1,5,4,0]]\n",
        "])\n",
        "\n",
        "# x2.shape = 2,4,3\n",
        "x2 = tf.constant([\n",
        "    [[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0]]\n",
        "\n",
        "    ,[[2,9,0,3],\n",
        "    [3,6,8,-2],\n",
        "    [1,3,5,0]]\n",
        "])\n",
        "\n",
        "eins_sum = tf.einsum('bij,bik -> j', x1,x2) # Summing up all elements in the columns\n",
        "print(\"Column wise addition\",eins_sum, \"\\n\")\n",
        "eins_sum = tf.einsum('bij,bik -> i', x1,x2) # Summing up all the elements in rows\n",
        "print(\"Row wise addition\",eins_sum,\"\\n\")\n",
        "eins_sum = tf.einsum('bij, bik -> b', x1,x2) # Summing up all the elements in batch wise\n",
        "print(\"Batch wise addition\",eins_sum,\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBae2Ix1qhRK",
        "outputId": "a0d11818-582d-4f2c-edf5-d9ae18985377"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column wise addition tf.Tensor([134 198 244 146], shape=(4,), dtype=int32) \n",
            "\n",
            "Row wise addition tf.Tensor([392 150 180], shape=(3,), dtype=int32) \n",
            "\n",
            "Batch wise addition tf.Tensor([361 361], shape=(2,), dtype=int32) \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Attention Mechanism\n",
        "\n",
        "# '''\n",
        "\n",
        "# batch_size: This is the number of samples in a single batch.\n",
        "# s_q || s_k || s_d || s_v: This represents the sequence length.\n",
        "# model_size: This is the size of the embedding or feature vector for each token in the sequence.  (e.g., 512, 1024, etc.).\n",
        "\n",
        "# Query\n",
        "# Q = batchsize,s_q,modelsize  - What are we asking? - Queries are used to compare against keys to compute attention scores.\n",
        "# # The attention mechanism calculates how much focus (or weight) each key should have in relation to each query.\n",
        "\n",
        "# Key\n",
        "# K = batchsize,s_k,modelsize # Key - What can we compare to? - Keys are used to compute the attention scores in relation to queries.\n",
        "# # Each key vector corresponds to a token in the sequence and is compared with the queries to determine how much attention a query should pay to a particular key.\n",
        "\n",
        "# Dimensionality\n",
        "# D = batchsize,s_d,modelsize # Dimensionality of model size\n",
        "# # It defines the number of features (or dimensions) used to represent each token in the sequence. Common values are 512, 1024, etc., depending on the model architecture.\n",
        "\n",
        "# Value\n",
        "# V = batchsize,s_v,modelsize # Value - the actual data that will be used to produce the output after attention.\n",
        "# # Values hold the actual content that the model needs to process after the attention mechanism has determined which parts of the input are most important.\n",
        "\n",
        "# Why Was It Needed?\n",
        "# Before attention mechanisms, models like RNNs and LSTMs would process input sequences sequentially and remember information through hidden states. However, they often struggled with long sequences because the model had to compress all information into a single fixed-length vector (the context vector) before making predictions. This led to information loss, especially for long sentences.\n",
        "\n",
        "# Problems with this approach:\n",
        "\n",
        "# Vanishing Gradient: In long sequences, important details from earlier parts of the sequence could be lost.\n",
        "# Fixed-size context: Compressing the entire input into one vector means losing nuanced or complex dependencies in longer sequences.\n",
        "\n",
        "# How the Attention Mechanism Solves It:\n",
        "# The attention mechanism allows the model to dynamically focus on relevant parts of the input sequence parallely during each step of the output generation, instead of relying on a fixed-size context vector. It computes attention scores to determine which tokens (words) in the input are most important for producing each output.\n",
        "\n",
        "# How it works:\n",
        "# Query (Q): What are we trying to focus on?\n",
        "# Key (K): What information from the input could be relevant?\n",
        "# Value (V): What actual content do we want to use for the output?\n",
        "# By comparing Q (query) with K (key), we get an attention score, which tells the model how much focus (weight) to give to each part of the input when computing the output.\n",
        "\n",
        "# '''"
      ],
      "metadata": {
        "id": "EOGXcVMyt6Kp"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = tf.random.normal((32,64,512))\n",
        "K = tf.random.normal((32,128,512))\n",
        "V = tf.random.uniform((32,200, 512))\n",
        "D = tf.random.uniform((32,512,512))\n",
        "\n",
        "# print(Q, \"\\n\")\n",
        "# print(K, \"\\n\")\n",
        "# print(V, \"\\n\")\n",
        "# print(D, \"\\n\")\n",
        "\n",
        "atten_e_shape = tf.einsum('bqm, bkm -> bqk', Q,K).shape\n",
        "print(atten_e_shape)\n",
        "atten_e = tf.einsum('bqm, bkm -> bqk', Q,K)\n",
        "print(atten_e)"
      ],
      "metadata": {
        "id": "PeEp8AV98A-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "A = tf.random.normal((2,4,6,4,2)) #bcij\n",
        "B = tf.random.normal((2,4,6,4,1)) #bcik\n",
        "print(A.shape)\n",
        "print(B.shape)\n",
        "# B^T * A\n",
        "C = tf.einsum('bcdik, bcdij-> bcdkj', B, A)\n",
        "print(C)\n",
        "print(C.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "850v3UiU-GTF",
        "outputId": "5d84102d-bb43-4eee-bf7b-3f9141cf3f47"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 4, 6, 4, 2)\n",
            "(2, 4, 6, 4, 1)\n",
            "tf.Tensor(\n",
            "[[[[[ 2.406842   -0.04552498]]\n",
            "\n",
            "   [[-0.31809473  0.9641504 ]]\n",
            "\n",
            "   [[ 1.116933    0.2236335 ]]\n",
            "\n",
            "   [[-0.3253295   0.6036027 ]]\n",
            "\n",
            "   [[-0.75789577 -1.5392165 ]]\n",
            "\n",
            "   [[ 0.05710427  1.588268  ]]]\n",
            "\n",
            "\n",
            "  [[[-2.2975512  -2.1292772 ]]\n",
            "\n",
            "   [[ 1.8588835  -0.6293715 ]]\n",
            "\n",
            "   [[-0.0529765  -0.18849061]]\n",
            "\n",
            "   [[-2.3053195   2.959392  ]]\n",
            "\n",
            "   [[ 1.4402726   0.4412862 ]]\n",
            "\n",
            "   [[ 0.22686625 -2.5896754 ]]]\n",
            "\n",
            "\n",
            "  [[[-1.7841402  -0.4344794 ]]\n",
            "\n",
            "   [[-1.4639571   0.08281344]]\n",
            "\n",
            "   [[-0.57325864 -2.0132964 ]]\n",
            "\n",
            "   [[ 0.06471872  1.133959  ]]\n",
            "\n",
            "   [[ 1.5523272  -0.6473383 ]]\n",
            "\n",
            "   [[-0.17417105 -1.9038916 ]]]\n",
            "\n",
            "\n",
            "  [[[ 1.9693708   0.08062457]]\n",
            "\n",
            "   [[-3.432708    2.900423  ]]\n",
            "\n",
            "   [[-0.49154678  1.2095435 ]]\n",
            "\n",
            "   [[-1.6264522  -1.0453346 ]]\n",
            "\n",
            "   [[ 1.460628   -3.0688553 ]]\n",
            "\n",
            "   [[ 2.3701985   2.0919256 ]]]]\n",
            "\n",
            "\n",
            "\n",
            " [[[[ 2.10514    -1.5235132 ]]\n",
            "\n",
            "   [[-1.9733005  -1.339136  ]]\n",
            "\n",
            "   [[-0.88734376  2.937897  ]]\n",
            "\n",
            "   [[ 0.58056843  1.1942999 ]]\n",
            "\n",
            "   [[-1.8686435  -2.9539752 ]]\n",
            "\n",
            "   [[ 1.1370962  -0.7454696 ]]]\n",
            "\n",
            "\n",
            "  [[[-0.01404822  0.7253064 ]]\n",
            "\n",
            "   [[ 1.4808601  -3.8171086 ]]\n",
            "\n",
            "   [[ 0.18935266  0.08265832]]\n",
            "\n",
            "   [[ 2.577744    3.269406  ]]\n",
            "\n",
            "   [[ 0.8469355  -0.7403309 ]]\n",
            "\n",
            "   [[-3.4487062   0.3806004 ]]]\n",
            "\n",
            "\n",
            "  [[[ 2.8351552   8.7544775 ]]\n",
            "\n",
            "   [[-1.281743    1.4563882 ]]\n",
            "\n",
            "   [[ 3.615679    0.84516907]]\n",
            "\n",
            "   [[ 0.77920145 -0.89400554]]\n",
            "\n",
            "   [[ 4.2388153   0.9659987 ]]\n",
            "\n",
            "   [[-1.8257765   0.4913594 ]]]\n",
            "\n",
            "\n",
            "  [[[ 0.1538608  -3.165174  ]]\n",
            "\n",
            "   [[-1.4565477   0.28940094]]\n",
            "\n",
            "   [[ 1.0808923  -3.0525868 ]]\n",
            "\n",
            "   [[-2.1807415   7.5057077 ]]\n",
            "\n",
            "   [[-1.8851537   0.70819753]]\n",
            "\n",
            "   [[-2.7296712  -1.863241  ]]]]], shape=(2, 4, 6, 1, 2), dtype=float32)\n",
            "(2, 4, 6, 1, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ETfoLmQNhH1T"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}